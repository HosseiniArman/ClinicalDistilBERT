{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as ts\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def train_model(dataset, tokenizer, model_path, save_path, num_epochs=3, batch_size=48):\n",
    "    \"\"\"\n",
    "    Train a masked language model using the Hugging Face Trainer.\n",
    "\n",
    "    Args:\n",
    "        dataset: The tokenized dataset.\n",
    "        tokenizer: The tokenizer for MLM.\n",
    "        model_path (str): Path to the pretrained model.\n",
    "        save_path (str): Path to save the trained model and checkpoints.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing model from {model_path}...\")\n",
    "    model = ts.AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "\n",
    "    # Calculate total trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_params / 1e6:.2f}M\")\n",
    "\n",
    "    # Data collator for MLM\n",
    "    data_collator = ts.DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_path + \"checkpoints\",\n",
    "        logging_steps=250,\n",
    "        overwrite_output_dir=True,\n",
    "        save_steps=2500,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=5e-5,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=5000,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        weight_decay=1e-4,\n",
    "        save_total_limit=5,\n",
    "        remove_unused_columns=True,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"Saving model to {save_path}final/model/...\")\n",
    "    trainer.save_model(save_path + \"final/model/\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
